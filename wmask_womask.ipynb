{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.python.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Каталог с данными для обучения\n",
    "train_dir = 'train2'\n",
    "# Каталог с данными для проверки\n",
    "val_dir = 'val2'\n",
    "# Каталог с данными для тестирования\n",
    "test_dir = 'test2'\n",
    "# Размеры изображения\n",
    "img_width, img_height = 224, 224\n",
    "# Размерность тензора на основе изображения для входных данных в нейронную сеть\n",
    "# backend Tensorflow, channels_last\n",
    "input_shape = (img_width, img_height, 3)\n",
    "# Количество эпох\n",
    "epochs = 10\n",
    "# Размер мини-выборки\n",
    "batch_size = 16\n",
    "# Количество изображений для обучения\n",
    "nb_train_samples = 1400\n",
    "# Количество изображений для проверки\n",
    "nb_validation_samples = 300\n",
    "# Количество изображений для тестирования\n",
    "nb_test_samples = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.applications.mobilenet.MobileNet(input_shape=input_shape, alpha=1.0, depth_multiplier=1, dropout=1e-3, include_top=True, weights=None, input_tensor=None, pooling=None, classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1. / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2789 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    classes=['0','1'],\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 598 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_generator = datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    classes=['0','1'],\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 597 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    classes=['0','1'],\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "87/87 [==============================] - 59s 673ms/step - loss: 0.4947 - acc: 0.7795 - val_loss: 0.2919 - val_acc: 0.9097\n",
      "Epoch 2/10\n",
      "87/87 [==============================] - 45s 516ms/step - loss: 0.1832 - acc: 0.9274 - val_loss: 0.2661 - val_acc: 0.9167\n",
      "Epoch 3/10\n",
      "87/87 [==============================] - 46s 525ms/step - loss: 0.1207 - acc: 0.9590 - val_loss: 0.4312 - val_acc: 0.8237\n",
      "Epoch 4/10\n",
      "87/87 [==============================] - 44s 501ms/step - loss: 0.0918 - acc: 0.9677 - val_loss: 0.1283 - val_acc: 0.9583\n",
      "Epoch 5/10\n",
      "87/87 [==============================] - 43s 497ms/step - loss: 0.0603 - acc: 0.9763 - val_loss: 0.4214 - val_acc: 0.8741\n",
      "Epoch 6/10\n",
      "87/87 [==============================] - 44s 500ms/step - loss: 0.0519 - acc: 0.9813 - val_loss: 0.1918 - val_acc: 0.9271\n",
      "Epoch 7/10\n",
      "87/87 [==============================] - 43s 497ms/step - loss: 0.0766 - acc: 0.9720 - val_loss: 0.1200 - val_acc: 0.9676\n",
      "Epoch 8/10\n",
      "87/87 [==============================] - 42s 488ms/step - loss: 0.0349 - acc: 0.9856 - val_loss: 0.1298 - val_acc: 0.9653\n",
      "Epoch 9/10\n",
      "87/87 [==============================] - 41s 468ms/step - loss: 0.0385 - acc: 0.9828 - val_loss: 0.0034 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "87/87 [==============================] - 41s 472ms/step - loss: 0.0361 - acc: 0.9849 - val_loss: 0.0691 - val_acc: 0.9826\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)\n",
    "\n",
    "model.save(\"model_v2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate_generator(test_generator, nb_test_samples // batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Аккуратность на тестовых данных: 98.26%\n"
     ]
    }
   ],
   "source": [
    "print(\"Аккуратность на тестовых данных: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n",
      "[[0.09174645 0.90825355]]\n",
      "Ответ :  1 , точность : 90.83 %\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread('test2/1/image.jpg')\n",
    "#cv2.imshow(\"img\", img)\n",
    "#cv2.waitKey(0) & 0xFF\n",
    "resized_img = cv2.resize(img, (224, 224))\n",
    "print(resized_img.shape)\n",
    "img = resized_img.reshape(1,224,224,3)\n",
    "img = img.astype('float32')\n",
    "img /= 255\n",
    "preds = model.predict(img)\n",
    "labels = [0, 1]\n",
    "ind = preds[0].argmax()\n",
    "print(preds)\n",
    "import numpy as np\n",
    "\n",
    "if round(preds[0][ind]*100,2)>75:\n",
    "    print(\"Ответ : \",labels[ind], \", точность :\", round(preds[0][ind]*100,2),\"%\" )\n",
    "else:\n",
    "    print(\"объект не распознан! Преблизительно - \", labels[ind], \"точность :\", round(preds[0][ind]*100,2),\"%\" ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognition(img):\n",
    "    resized_img = cv2.resize(img, (224, 224))\n",
    "    print(resized_img.shape)\n",
    "    img = resized_img.reshape(1,224,224,3)\n",
    "    img = img.astype('float32')\n",
    "    img /= 255\n",
    "    preds = model.predict(img)\n",
    "    labels = [0,1]\n",
    "    ind = preds[0].argmax()\n",
    "    print(\"Ответ : \",labels[ind], \", точность :\", round(preds[0][ind]*100,2),\"%\" )\n",
    "    acc = round(preds[0][ind]*100,2)\n",
    "    if acc > 75:\n",
    "        res = \"\"+str(labels[ind])+\", \"+str(acc) +\"%\"\n",
    "    else:\n",
    "        res= \"0, 0%\"\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import os \n",
    "\n",
    "cam = cv2.VideoCapture(0) \n",
    "cam.set(3, 640) # set video width \n",
    "cam.set(4, 480) # set video height \n",
    "\n",
    "face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml') \n",
    "\n",
    "# For each person, enter one numeric face id \n",
    "face_id = input('\\n enter user id end press ==> ') \n",
    "\n",
    "print(\"\\n [INFO] Initializing face capture. Look the camera and wait ...\") \n",
    "# Initialize individual sampling face count \n",
    "count = 0 \n",
    "\n",
    "while(True): \n",
    "    ret, img = cam.read() \n",
    "    img = cv2.flip(img, 1) # flip video image vertically \n",
    "    #gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) \n",
    "    faces = face_detector.detectMultiScale(img, 1.3, 5) \n",
    "\n",
    "    for (x,y,w,h) in faces: \n",
    "        rrec = cv2.rectangle(img, (x,y), (x+w,y+h), (255,0,0), 2) \n",
    "        #count += 1 \n",
    "        #dim = (100, 100) \n",
    "        #resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA) \n",
    "        # Save the captured image into the datasets folder \n",
    "        #cv2.imwrite(\"test1/User.\" + str(face_id) + '.' + str(count) + \".jpg\", img[y:y+h,x:x+w]) \n",
    "        crop = img[y:y+h,x:x+w]\n",
    "        text = recognition(crop)\n",
    "        cv2.putText(rrec,text,(100,100), cv2.FONT_HERSHEY_SIMPLEX, 1,(0,0,255),1,cv2.LINE_AA)\n",
    "    cv2.imshow('image', img) \n",
    "\n",
    "    k = cv2.waitKey(100) & 0xff # Press 'ESC' for exiting video \n",
    "    if k == 27: \n",
    "        break \n",
    "   \n",
    "\n",
    "# Do a bit of cleanup \n",
    "print(\"\\n [INFO] Exiting and cleanup stuff\") \n",
    "cam.release() \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
